"""
Security Vulnerability Scanning and Testing.

Tests for common security vulnerabilities including injection attacks,
XSS, CSRF, insecure configurations, and API security issues.
"""

import pytest
import json
import re
import time
from datetime import datetime, timedelta
from unittest.mock import Mock, patch, MagicMock
from typing import Dict, List, Any
import hashlib
import secrets


class TestInjectionVulnerabilities:
    """Test injection vulnerability detection and prevention."""
    
    def test_sql_injection_prevention(self, vulnerability_test_payloads):
        """Test SQL injection prevention mechanisms."""
        sql_payloads = vulnerability_test_payloads["sql_injection"]
        
        def simulate_database_query(user_input, use_parameterized=True):
            """Simulate database query with/without parameterization."""
            if use_parameterized:
                # Parameterized query (safe)
                # In real implementation, this would use proper ORM or prepared statements
                sanitized_input = user_input.replace("'", "''")  # Basic escaping
                return {
                    "query": f"SELECT * FROM customers WHERE name = ?",
                    "parameters": [sanitized_input],
                    "vulnerable": False,
                    "result": "SAFE_QUERY_EXECUTED"
                }
            else:
                # Direct string concatenation (vulnerable)
                query = f"SELECT * FROM customers WHERE name = '{user_input}'"
                
                # Check for SQL injection patterns
                injection_patterns = [
                    r"';.*DROP.*TABLE.*",
                    r"'.*OR.*'1'='1",
                    r"'.*UNION.*SELECT.*",
                    r"admin'--"
                ]
                
                vulnerable = any(re.search(pattern, query, re.IGNORECASE) for pattern in injection_patterns)
                
                return {
                    "query": query,
                    "parameters": None,
                    "vulnerable": vulnerable,
                    "result": "POTENTIALLY_MALICIOUS_QUERY" if vulnerable else "QUERY_EXECUTED"
                }
        
        # Test with parameterized queries (should be safe)
        for payload in sql_payloads:
            safe_result = simulate_database_query(payload, use_parameterized=True)
            assert not safe_result["vulnerable"], f"Parameterized query should be safe for payload: {payload}"
        
        # Test with direct concatenation (should detect vulnerabilities)
        vulnerable_count = 0
        for payload in sql_payloads:
            unsafe_result = simulate_database_query(payload, use_parameterized=False)
            if unsafe_result["vulnerable"]:
                vulnerable_count += 1
        
        # Should detect at least some SQL injection attempts
        assert vulnerable_count > 0, "Should detect SQL injection vulnerabilities"
    
    def test_nosql_injection_prevention(self):
        """Test NoSQL injection prevention."""
        # MongoDB-style injection payloads
        nosql_payloads = [
            {"$ne": None},
            {"$gt": ""},
            {"$where": "function() { return true; }"},
            {"$regex": ".*"},
            {"username": {"$ne": None}, "password": {"$ne": None}}
        ]
        
        def validate_nosql_input(user_input):
            """Validate NoSQL input for injection attempts."""
            if isinstance(user_input, dict):
                # Check for MongoDB operators
                mongodb_operators = ["$ne", "$gt", "$lt", "$gte", "$lte", "$in", "$nin", "$where", "$regex", "$exists"]
                
                for key in user_input.keys():
                    if key.startswith("$") and key in mongodb_operators:
                        return {"valid": False, "reason": f"Prohibited operator: {key}"}
                
                # Recursively check nested objects
                for value in user_input.values():
                    if isinstance(value, dict):
                        nested_result = validate_nosql_input(value)
                        if not nested_result["valid"]:
                            return nested_result
            
            return {"valid": True, "reason": None}
        
        # Test NoSQL injection detection
        for payload in nosql_payloads:
            validation_result = validate_nosql_input(payload)
            if isinstance(payload, dict) and any(key.startswith("$") for key in payload.keys()):
                assert not validation_result["valid"], f"Should detect NoSQL injection in payload: {payload}"
    
    def test_command_injection_prevention(self, vulnerability_test_payloads):
        """Test command injection prevention."""
        command_payloads = vulnerability_test_payloads["command_injection"]
        
        def validate_system_command_input(user_input):
            """Validate input for system command execution."""
            # Dangerous command patterns
            dangerous_patterns = [
                r';.*ls.*',
                r'\|.*cat.*',
                r'&&.*rm.*',
                r'`.*`',
                r'\$\(.*\)',
                r'\.\./',
                r'/etc/passwd',
                r'/bin/',
                r'whoami',
                r'id\s*$'
            ]
            
            for pattern in dangerous_patterns:
                if re.search(pattern, user_input, re.IGNORECASE):
                    return {
                        "safe": False,
                        "reason": f"Dangerous pattern detected: {pattern}",
                        "input": user_input
                    }
            
            return {"safe": True, "reason": None, "input": user_input}
        
        # Test command injection detection
        detected_count = 0
        for payload in command_payloads:
            validation_result = validate_system_command_input(payload)
            if not validation_result["safe"]:
                detected_count += 1
        
        # Should detect most command injection attempts
        assert detected_count >= len(command_payloads) * 0.7, "Should detect majority of command injection attempts"
    
    def test_ldap_injection_prevention(self):
        """Test LDAP injection prevention."""
        # LDAP injection payloads
        ldap_payloads = [
            "admin)(|(password=*))",
            "*)(uid=*))(|(uid=*",
            "admin)(&(password=*))",
            "*)(&(objectClass=*)",
            "admin)(cn=*)"
        ]
        
        def sanitize_ldap_input(user_input):
            """Sanitize input for LDAP queries."""
            # LDAP special characters that need escaping
            ldap_special_chars = {
                '(': '\\28',
                ')': '\\29',
                '*': '\\2A',
                '\\': '\\5C',
                '\x00': '\\00'
            }
            
            sanitized = user_input
            for char, escaped in ldap_special_chars.items():
                sanitized = sanitized.replace(char, escaped)
            
            return {
                "original": user_input,
                "sanitized": sanitized,
                "safe": sanitized != user_input or not any(char in user_input for char in ldap_special_chars.keys())
            }
        
        # Test LDAP injection prevention
        for payload in ldap_payloads:
            result = sanitize_ldap_input(payload)
            # Original payload should contain dangerous characters
            assert not result["safe"] or result["sanitized"] != result["original"], \
                f"Should sanitize LDAP injection payload: {payload}"


class TestCrossSiteScriptingXSS:
    """Test XSS vulnerability detection and prevention."""
    
    def test_reflected_xss_prevention(self, vulnerability_test_payloads):
        """Test reflected XSS prevention."""
        xss_payloads = vulnerability_test_payloads["xss"]
        
        def sanitize_html_output(user_input):
            """Sanitize user input for HTML output."""
            # HTML entities to escape
            html_entities = {
                '<': '&lt;',
                '>': '&gt;',
                '"': '&quot;',
                "'": '&#x27;',
                '&': '&amp;',
                '/': '&#x2F;'
            }
            
            sanitized = user_input
            for char, entity in html_entities.items():
                sanitized = sanitized.replace(char, entity)
            
            # Additional sanitization for javascript: protocol and event handlers
            sanitized = sanitized.replace('javascript:', 'javascript&#x3A;')
            
            # Remove event handlers
            import re
            sanitized = re.sub(r'on\w+\s*=\s*["\'][^"\']*["\']', '', sanitized, flags=re.IGNORECASE)
            sanitized = re.sub(r'on\w+\s*=\s*[^"\'\s>]+', '', sanitized, flags=re.IGNORECASE)
            
            # Check for remaining dangerous patterns after HTML encoding
            dangerous_patterns = [
                r'<script.*?>',
                r'javascript:',
                r'on\w+\s*=',
                r'<iframe.*?>',
                r'<object.*?>'
            ]
            
            still_dangerous = any(re.search(pattern, sanitized, re.IGNORECASE) for pattern in dangerous_patterns)
            
            return {
                "original": user_input,
                "sanitized": sanitized,
                "safe": not still_dangerous
            }
        
        # Test XSS prevention
        for payload in xss_payloads:
            result = sanitize_html_output(payload)
            assert result["safe"], f"Should sanitize XSS payload: {payload}"
            assert result["sanitized"] != result["original"], f"Should modify dangerous payload: {payload}"
    
    def test_stored_xss_prevention(self):
        """Test stored XSS prevention."""
        # Simulate stored data that could contain XSS
        stored_data_samples = [
            {"user_comment": "<script>alert('stored xss')</script>"},
            {"profile_bio": "Hello <img src=x onerror=alert('xss')>"},
            {"company_name": "ABC Corp <svg onload=alert('xss')>"},
            {"address": "123 Main St <iframe src=javascript:alert('xss')></iframe>"}
        ]
        
        def validate_stored_content(content):
            """Validate content before storing to prevent XSS."""
            validation_result = {
                "content": content,
                "safe": True,
                "violations": []
            }
            
            # XSS patterns to detect
            xss_patterns = [
                (r'<script.*?>', "Script tag detected"),
                (r'javascript:', "JavaScript protocol detected"),
                (r'on\w+\s*=', "Event handler detected"),
                (r'<iframe.*?>', "Iframe tag detected"),
                (r'<object.*?>', "Object tag detected"),
                (r'<embed.*?>', "Embed tag detected"),
                (r'<svg.*?onload.*?>', "SVG with onload detected")
            ]
            
            for pattern, description in xss_patterns:
                if re.search(pattern, content, re.IGNORECASE):
                    validation_result["safe"] = False
                    validation_result["violations"].append(description)
            
            return validation_result
        
        # Test stored XSS detection
        for data_sample in stored_data_samples:
            for field_name, field_value in data_sample.items():
                validation = validate_stored_content(field_value)
                assert not validation["safe"], f"Should detect XSS in {field_name}: {field_value}"
                assert len(validation["violations"]) > 0
    
    def test_dom_xss_prevention(self):
        """Test DOM-based XSS prevention patterns."""
        # Simulate client-side code patterns that could lead to DOM XSS
        dom_xss_scenarios = [
            {
                "code": "document.getElementById('output').innerHTML = userInput;",
                "vulnerable": True,
                "reason": "Direct innerHTML assignment"
            },
            {
                "code": "document.getElementById('output').textContent = userInput;",
                "vulnerable": False,
                "reason": "Safe textContent assignment"
            },
            {
                "code": "eval(userInput);",
                "vulnerable": True,
                "reason": "Direct eval usage"
            },
            {
                "code": "setTimeout(userInput, 1000);",
                "vulnerable": True,
                "reason": "setTimeout with string parameter"
            }
        ]
        
        def analyze_dom_xss_risk(code_snippet):
            """Analyze code for DOM XSS risks."""
            high_risk_patterns = [
                r'\.innerHTML\s*=',
                r'eval\s*\(',
                r'setTimeout\s*\(\s*[^,]*userInput',
                r'setInterval\s*\(\s*[^,]*userInput',
                r'document\.write\s*\(',
                r'\.outerHTML\s*='
            ]
            
            medium_risk_patterns = [
                r'\.src\s*=.*userInput',
                r'\.href\s*=.*userInput',
                r'location\s*=.*userInput'
            ]
            
            risk_level = "LOW"
            detected_patterns = []
            
            for pattern in high_risk_patterns:
                if re.search(pattern, code_snippet, re.IGNORECASE):
                    risk_level = "HIGH"
                    detected_patterns.append(pattern)
            
            if risk_level != "HIGH":
                for pattern in medium_risk_patterns:
                    if re.search(pattern, code_snippet, re.IGNORECASE):
                        risk_level = "MEDIUM"
                        detected_patterns.append(pattern)
            
            return {
                "code": code_snippet,
                "risk_level": risk_level,
                "patterns": detected_patterns,
                "vulnerable": risk_level in ["HIGH", "MEDIUM"]
            }
        
        # Test DOM XSS analysis
        for scenario in dom_xss_scenarios:
            analysis = analyze_dom_xss_risk(scenario["code"])
            assert analysis["vulnerable"] == scenario["vulnerable"], \
                f"DOM XSS analysis mismatch for: {scenario['code']}"


class TestCSRFProtection:
    """Test Cross-Site Request Forgery (CSRF) protection."""
    
    def test_csrf_token_validation(self):
        """Test CSRF token validation mechanisms."""
        def generate_csrf_token(session_id):
            """Generate CSRF token for session."""
            secret_key = "csrf_secret_key_12345"
            token_data = f"{session_id}:{int(time.time())}"
            token_hash = hashlib.sha256(f"{secret_key}:{token_data}".encode()).hexdigest()
            return f"{token_data}:{token_hash}"
        
        def validate_csrf_token(token, session_id, max_age=3600):
            """Validate CSRF token."""
            try:
                parts = token.split(':')
                if len(parts) != 3:
                    return {"valid": False, "reason": "Invalid token format"}
                
                token_session_id, timestamp, token_hash = parts
                
                # Verify session ID matches
                if token_session_id != session_id:
                    return {"valid": False, "reason": "Session ID mismatch"}
                
                # Verify token age
                token_time = int(timestamp)
                current_time = int(time.time())
                if current_time - token_time > max_age:
                    return {"valid": False, "reason": "Token expired"}
                
                # Verify token hash
                secret_key = "csrf_secret_key_12345"
                expected_data = f"{token_session_id}:{timestamp}"
                expected_hash = hashlib.sha256(f"{secret_key}:{expected_data}".encode()).hexdigest()
                
                if token_hash != expected_hash:
                    return {"valid": False, "reason": "Invalid token hash"}
                
                return {"valid": True, "reason": None}
                
            except Exception as e:
                return {"valid": False, "reason": f"Token validation error: {str(e)}"}
        
        # Test CSRF token generation and validation
        session_id = "session_12345"
        csrf_token = generate_csrf_token(session_id)
        
        # Valid token should pass
        validation_result = validate_csrf_token(csrf_token, session_id)
        assert validation_result["valid"], "Valid CSRF token should pass validation"
        
        # Invalid session ID should fail
        invalid_session_result = validate_csrf_token(csrf_token, "wrong_session")
        assert not invalid_session_result["valid"], "CSRF token with wrong session should fail"
        
        # Malformed token should fail
        malformed_token = "invalid:token:format:extra"
        malformed_result = validate_csrf_token(malformed_token, session_id)
        assert not malformed_result["valid"], "Malformed CSRF token should fail"
    
    def test_same_site_cookie_protection(self):
        """Test SameSite cookie protection against CSRF."""
        cookie_configurations = [
            {
                "name": "session_cookie",
                "value": "session_value_123",
                "same_site": "Strict",
                "secure": True,
                "http_only": True,
                "csrf_protection_level": "HIGH"
            },
            {
                "name": "tracking_cookie",
                "value": "tracking_value_456",
                "same_site": "Lax",
                "secure": True,
                "http_only": False,
                "csrf_protection_level": "MEDIUM"
            },
            {
                "name": "legacy_cookie",
                "value": "legacy_value_789",
                "same_site": None,
                "secure": False,
                "http_only": False,
                "csrf_protection_level": "LOW"
            }
        ]
        
        def evaluate_cookie_csrf_protection(cookie_config):
            """Evaluate CSRF protection level of cookie configuration."""
            protection_score = 0
            
            # SameSite attribute
            if cookie_config["same_site"] == "Strict":
                protection_score += 40
            elif cookie_config["same_site"] == "Lax":
                protection_score += 20
            
            # Secure flag
            if cookie_config["secure"]:
                protection_score += 20
            
            # HttpOnly flag
            if cookie_config["http_only"]:
                protection_score += 20
            
            # Additional security headers (simulated)
            protection_score += 20  # Assume proper security headers
            
            if protection_score >= 80:
                level = "HIGH"
            elif protection_score >= 50:
                level = "MEDIUM"
            else:
                level = "LOW"
            
            return {
                "cookie_name": cookie_config["name"],
                "protection_score": protection_score,
                "protection_level": level,
                "csrf_resistant": protection_score >= 60
            }
        
        # Test cookie CSRF protection evaluation
        for cookie_config in cookie_configurations:
            protection_eval = evaluate_cookie_csrf_protection(cookie_config)
            expected_level = cookie_config["csrf_protection_level"]
            
            assert protection_eval["protection_level"] == expected_level, \
                f"Cookie {cookie_config['name']} protection level mismatch"
    
    def test_referer_header_validation(self):
        """Test Referer header validation for CSRF protection."""
        def validate_referer_header(referer, allowed_origins):
            """Validate Referer header against allowed origins."""
            if not referer:
                return {"valid": False, "reason": "Missing Referer header"}
            
            try:
                # Extract origin from referer
                if referer.startswith("https://"):
                    referer_origin = referer.split("/")[2]
                elif referer.startswith("http://"):
                    referer_origin = referer.split("/")[2]
                else:
                    return {"valid": False, "reason": "Invalid Referer format"}
                
                # Check against allowed origins
                if referer_origin in allowed_origins:
                    return {"valid": True, "reason": None}
                else:
                    return {"valid": False, "reason": f"Referer origin {referer_origin} not in allowed list"}
                    
            except Exception as e:
                return {"valid": False, "reason": f"Referer validation error: {str(e)}"}
        
        # Test referer validation
        allowed_origins = ["api.example.com", "app.example.com", "localhost:3000"]
        
        test_cases = [
            {"referer": "https://api.example.com/dashboard", "should_pass": True},
            {"referer": "https://malicious.com/attack", "should_pass": False},
            {"referer": None, "should_pass": False},
            {"referer": "invalid-url", "should_pass": False}
        ]
        
        for test_case in test_cases:
            result = validate_referer_header(test_case["referer"], allowed_origins)
            if test_case["should_pass"]:
                assert result["valid"], f"Referer {test_case['referer']} should be valid"
            else:
                assert not result["valid"], f"Referer {test_case['referer']} should be invalid"


class TestAPISecurityVulnerabilities:
    """Test API-specific security vulnerabilities."""
    
    def test_rate_limiting_protection(self, rate_limit_config):
        """Test rate limiting protection mechanisms."""
        # Persistent rate limit store for the test
        rate_limit_store = {}
        
        def simulate_rate_limiter(client_id, requests_per_minute):
            """Simulate rate limiting for API requests."""
            current_time = int(time.time())
            minute_window = current_time // 60
            
            if client_id not in rate_limit_store:
                rate_limit_store[client_id] = {}
            
            if minute_window not in rate_limit_store[client_id]:
                rate_limit_store[client_id][minute_window] = 0
            
            rate_limit_store[client_id][minute_window] += 1
            current_requests = rate_limit_store[client_id][minute_window]
            
            return {
                "client_id": client_id,
                "current_requests": current_requests,
                "limit": requests_per_minute,
                "allowed": current_requests <= requests_per_minute,
                "reset_time": (minute_window + 1) * 60
            }
        
        # Test rate limiting
        client_id = "test_client_001"
        limit = rate_limit_config["requests_per_minute"]
        
        # Simulate requests within limit
        for i in range(limit):
            result = simulate_rate_limiter(client_id, limit)
            assert result["allowed"], f"Request {i+1} should be allowed within limit"
        
        # Simulate request exceeding limit
        exceeded_result = simulate_rate_limiter(client_id, limit)
        assert not exceeded_result["allowed"], "Request exceeding limit should be blocked"
    
    def test_input_validation_bypass(self):
        """Test input validation bypass attempts."""
        # API input validation scenarios
        validation_scenarios = [
            {
                "endpoint": "/api/customers",
                "input": {"name": "John Doe", "age": 30},
                "expected_valid": True
            },
            {
                "endpoint": "/api/customers",
                "input": {"name": "A" * 1000, "age": 30},  # Extremely long name
                "expected_valid": False
            },
            {
                "endpoint": "/api/customers",
                "input": {"name": "John", "age": -5},  # Invalid age
                "expected_valid": False
            },
            {
                "endpoint": "/api/customers",
                "input": {"name": None, "age": 30},  # Missing required field
                "expected_valid": False
            }
        ]
        
        def validate_api_input(endpoint, input_data):
            """Validate API input data."""
            validation_rules = {
                "/api/customers": {
                    "name": {"required": True, "type": str, "max_length": 100},
                    "age": {"required": True, "type": int, "min_value": 0, "max_value": 150}
                }
            }
            
            if endpoint not in validation_rules:
                return {"valid": False, "errors": ["Unknown endpoint"]}
            
            rules = validation_rules[endpoint]
            errors = []
            
            for field_name, field_rules in rules.items():
                field_value = input_data.get(field_name)
                
                # Required field check
                if field_rules.get("required", False) and field_value is None:
                    errors.append(f"Field {field_name} is required")
                    continue
                
                if field_value is not None:
                    # Type check
                    expected_type = field_rules.get("type")
                    if expected_type and not isinstance(field_value, expected_type):
                        errors.append(f"Field {field_name} must be of type {expected_type.__name__}")
                    
                    # String length check
                    if isinstance(field_value, str):
                        max_length = field_rules.get("max_length")
                        if max_length and len(field_value) > max_length:
                            errors.append(f"Field {field_name} exceeds maximum length of {max_length}")
                    
                    # Numeric range check
                    if isinstance(field_value, (int, float)):
                        min_value = field_rules.get("min_value")
                        max_value = field_rules.get("max_value")
                        if min_value is not None and field_value < min_value:
                            errors.append(f"Field {field_name} must be at least {min_value}")
                        if max_value is not None and field_value > max_value:
                            errors.append(f"Field {field_name} must be at most {max_value}")
            
            return {"valid": len(errors) == 0, "errors": errors}
        
        # Test input validation
        for scenario in validation_scenarios:
            result = validate_api_input(scenario["endpoint"], scenario["input"])
            assert result["valid"] == scenario["expected_valid"], \
                f"Validation mismatch for input: {scenario['input']}"
    
    def test_authentication_bypass_attempts(self):
        """Test authentication bypass vulnerability detection."""
        # Authentication bypass scenarios
        bypass_attempts = [
            {
                "method": "JWT_NONE_ALGORITHM",
                "token": "eyJ0eXAiOiJKV1QiLCJhbGciOiJub25lIn0.eyJzdWIiOiJhZG1pbiJ9.",
                "should_block": True
            },
            {
                "method": "EMPTY_PASSWORD",
                "credentials": {"username": "admin", "password": ""},
                "should_block": True
            },
            {
                "method": "SQL_INJECTION_USERNAME",
                "credentials": {"username": "admin' OR '1'='1", "password": "password"},
                "should_block": True
            },
            {
                "method": "VALID_CREDENTIALS",
                "credentials": {"username": "valid_user", "password": "valid_password"},
                "should_block": False
            }
        ]
        
        def detect_authentication_bypass(attempt):
            """Detect authentication bypass attempts."""
            detection_result = {
                "method": attempt["method"],
                "blocked": False,
                "reason": None
            }
            
            if attempt["method"] == "JWT_NONE_ALGORITHM":
                # Check for 'none' algorithm in JWT header
                token = attempt["token"]
                try:
                    # Decode JWT header without verification
                    import base64
                    header_b64 = token.split('.')[0]
                    # Add padding if needed
                    header_b64 += '=' * (4 - len(header_b64) % 4)
                    header = json.loads(base64.b64decode(header_b64))
                    if header.get("alg", "").lower() == "none":
                        detection_result["blocked"] = True
                        detection_result["reason"] = "JWT 'none' algorithm detected"
                except:
                    # If we can't decode, treat as suspicious
                    if "none" in token.lower():
                        detection_result["blocked"] = True
                        detection_result["reason"] = "JWT 'none' algorithm detected"
            
            elif attempt["method"] == "EMPTY_PASSWORD":
                credentials = attempt["credentials"]
                if not credentials.get("password"):
                    detection_result["blocked"] = True
                    detection_result["reason"] = "Empty password not allowed"
            
            elif attempt["method"] == "SQL_INJECTION_USERNAME":
                credentials = attempt["credentials"]
                username = credentials.get("username", "")
                sql_patterns = ["'", "OR", "1=1", "--", "/*", "*/"]
                if any(pattern.lower() in username.lower() for pattern in sql_patterns):
                    detection_result["blocked"] = True
                    detection_result["reason"] = "SQL injection pattern in username"
            
            return detection_result
        
        # Test authentication bypass detection
        for attempt in bypass_attempts:
            result = detect_authentication_bypass(attempt)
            assert result["blocked"] == attempt["should_block"], \
                f"Authentication bypass detection failed for method: {attempt['method']}"
    
    def test_insecure_direct_object_references(self):
        """Test Insecure Direct Object References (IDOR) vulnerabilities."""
        # Simulate user access scenarios
        access_scenarios = [
            {
                "user_id": "user_001",
                "requested_resource": "/api/customers/cust_001",
                "user_permissions": ["read_own_data"],
                "resource_owner": "user_001",
                "should_allow": True
            },
            {
                "user_id": "user_001", 
                "requested_resource": "/api/customers/cust_002",
                "user_permissions": ["read_own_data"],
                "resource_owner": "user_002",
                "should_allow": False  # IDOR vulnerability if allowed
            },
            {
                "user_id": "admin_001",
                "requested_resource": "/api/customers/cust_002",
                "user_permissions": ["read_all_data"],
                "resource_owner": "user_002",
                "should_allow": True  # Admin can access all
            }
        ]
        
        def check_resource_access_authorization(scenario):
            """Check if user is authorized to access resource."""
            user_id = scenario["user_id"]
            resource_path = scenario["requested_resource"]
            permissions = scenario["user_permissions"]
            resource_owner = scenario["resource_owner"]
            
            # Extract resource ID from path
            resource_id = resource_path.split("/")[-1]
            
            # Authorization logic
            if "read_all_data" in permissions:
                return {"authorized": True, "reason": "Admin access"}
            
            if "read_own_data" in permissions:
                if resource_owner == user_id:
                    return {"authorized": True, "reason": "Own resource access"}
                else:
                    return {"authorized": False, "reason": "IDOR: Accessing other user's resource"}
            
            return {"authorized": False, "reason": "No permissions"}
        
        # Test IDOR protection
        for scenario in access_scenarios:
            result = check_resource_access_authorization(scenario)
            assert result["authorized"] == scenario["should_allow"], \
                f"IDOR check failed for user {scenario['user_id']} accessing {scenario['requested_resource']}"